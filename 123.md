本文来自基础学科领域顶级期刊 Proceedings of the National Academy of Sciences of the 
United States of America （《美国科学院院报》）。

作者Christian Ramiro，是UC Berkeley大学的一名大二学生，专业是计算机科学和认知科学。

该文被《外语教学与研究》的综述文章《基于大数据的语义实证研究新进展》引用。



问题：

Why is polysemy a dominant strategy in lexical evolution, and how do words develop new senses over time?



它在解决什么问题：

预测历史上单词词义出现的顺序



FACE

![image-20210427231558351](image-20210427231558351.png)

+   Random Algorithm

    新词义的出现是随机的

+   Exemplar Algorithm

    新词义的出现和已有词义的平均值相关

+   Prototype Algorithm

    新词义的出现和已有词义中最具代表性的词义（即Prototype）相关，最具代表性的词义与其他所有词义的平均距离最小。

+   Progenitor Algorithm

    Prototype Algorithm的静态版本，它认为新词义的出现总是与最早的词义相关。

+   Local Algorithm

    新词义的出现和已有词义中最后出现的相关。

+   Nearest-Neighbor Chaining Algorithm（Proposed Algorithm）

    新词义的出现遵循最小生成树的Prim求解法则，即总是朝着使词义网认知成本最低的方向发展。



作者用模拟的方法比较，我个人认为没有必要。因为最小生成树的cost当然就是最小的，这个在理论上就有证明，无需做实验验证。



结论1

Over the history of English, new senses have been more frequently expressed via reuse of existing words than via new word forms.